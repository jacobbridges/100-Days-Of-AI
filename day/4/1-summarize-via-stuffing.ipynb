{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summarize via Stuffing\n",
    "\n",
    "\"Stuffing\" is a silly as it sounds -- essentially you \"stuff\" the large document into the input prompt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install langchain\n",
    "!pip install langchain-community\n",
    "!pip install gpt4all"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.llms.gpt4all import GPT4All\n",
    "\n",
    "# mistral-7b download available from the gpt4all website. Use the \"Model Explorer\"\n",
    "# https://gpt4all.io/\n",
    "llm = GPT4All(\n",
    "    model=\"../../models/mistral-7b-openorca.Q4_0.gguf\",\n",
    "    max_tokens=1024,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the data\n",
    "\n",
    "I will be working with two datasets:\n",
    "\n",
    "- `data/small-document.txt` - Paragraph from [this article](https://www.nature.com/articles/s41467-017-01082-6).\n",
    "- `data/large-document.txt` - Full transcript of [this podcast](https://anchor.fm/s/74aab30/podcast/play/1593261/https%3A%2F%2Fd3ctxlq1ktw2nl.cloudfront.net%2Fproduction%2F2018-9-22%2F5313967-44100-1-ae7cde1436c24.mp3)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "SMALL_DOC = Path(\"./data/small-document.txt\").read_text()\n",
    "LARGE_DOC = Path(\"./data/large-document.txt\").read_text()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stuffing with a simple chain\n",
    "\n",
    "This is how we can achieve stuffing with a `PromptTemplate` and llm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summary: \n",
      "\n",
      " RNA is recognized as a powerful tool in controlling gene expression and engineering synthetic cellular functions due to its ability to create specific structures that can interact with cellular machinery. This has led to the development of RNA regulators capable of controlling almost every aspect of gene expression, which can be further controlled by small RNAs or ligand binding. The potential for using nucleic acid design algorithms to create de novo RNA regulators offers a major advantage over protein-based regulation and promises significant advancements in synthetic biology. \n",
      "\n",
      "\n",
      "Number of words in original document:  190\n",
      "Number of words in summary:  87\n"
     ]
    }
   ],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "prompt = PromptTemplate.from_template(\n",
    "    \"Here is the full document: \\n\\n\"\n",
    "    '{document}\\n\\n'\n",
    "    \"And here is the concise summary: \\n\\n\"\n",
    ")\n",
    "\n",
    "summary = (prompt | llm).invoke({\n",
    "    \"document\": SMALL_DOC\n",
    "})\n",
    "\n",
    "print(\"Summary: \\n\\n\", summary, \"\\n\\n\")\n",
    "print(\"Number of words in original document: \", len(SMALL_DOC.split()))\n",
    "print(\"Number of words in summary: \", len(summary.split()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Attempting to stuff the larger document should produce an error, since the document is larger than the allowed token context. I will learn more about tokenization in a future lesson -- for now it's a bit of trial and error. Let's see what that error looks like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summary: \n",
      "\n",
      " ERROR: The prompt size exceeds the context window size and cannot be processed. \n",
      "\n",
      "\n",
      "Number of words in original document:  7251\n",
      "Number of words in summary:  13\n"
     ]
    }
   ],
   "source": [
    "summary = (prompt | llm).invoke({\n",
    "    \"document\": LARGE_DOC\n",
    "})\n",
    "\n",
    "print(\"Summary: \\n\\n\", summary, \"\\n\\n\")\n",
    "print(\"Number of words in original document: \", len(LARGE_DOC.split()))\n",
    "print(\"Number of words in summary: \", len(summary.split()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Huh. Worth noting that this did not raise an error in Python -- instead the chain responded with error text. Curious how I should go about catching those errors properly in the future?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Langchain & Summarization Chains\n",
    "\n",
    "It is worth noting that LangChain has a builtin chain for summarizing: [StuffDocumentsChain](https://api.python.langchain.com/en/latest/chains/langchain.chains.combine_documents.stuff.StuffDocumentsChain.html). I followed [this guide](https://www.comet.com/site/blog/mastering-document-chains-in-langchain/) to write the following code.\n",
    "\n",
    "> ðŸŒž Side Note: This code covers several concepts which I have not learned about yet. Once I have written lessons on them, I will come back and provide links to those learnings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RNA is recognized as a powerful tool in controlling gene expression and engineering synthetic cellular functions due to its ability to create specific structures that can interact with cellular machinery. This has led to the development of RNA regulators capable of controlling almost every aspect of gene expression, which can be further controlled by small RNAs or ligand binding. The potential for using nucleic acid design algorithms to create de novo RNA regulators offers a major advantage over protein-based regulation and promises significant advancements in synthetic biology.\n"
     ]
    }
   ],
   "source": [
    "from langchain.chains.summarize import load_summarize_chain\n",
    "from langchain.docstore.document import Document\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "\n",
    "# splits the text based on a character\n",
    "text_splitter = CharacterTextSplitter(\n",
    "    separator='\\n',\n",
    "    chunk_size = 512,\n",
    "    chunk_overlap=20,\n",
    "    length_function=len)\n",
    "\n",
    "small_doc_chunks = text_splitter.split_text(SMALL_DOC)\n",
    "\n",
    "docs = [Document(page_content=t) for t in small_doc_chunks]\n",
    "\n",
    "stuff_chain = load_summarize_chain(\n",
    "    llm,\n",
    "    chain_type=\"stuff\",\n",
    "    prompt=prompt,\n",
    "    document_variable_name=\"document\",\n",
    ")\n",
    "\n",
    "output_summary = stuff_chain.run(docs)\n",
    "print(output_summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ERROR: The prompt size exceeds the context window size and cannot be processed.\n"
     ]
    }
   ],
   "source": [
    "large_doc_chunks = text_splitter.split_text(LARGE_DOC)\n",
    "docs = [Document(page_content=t) for t in large_doc_chunks]\n",
    "\n",
    "output_summary = stuff_chain.run(docs)\n",
    "print(output_summary)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
