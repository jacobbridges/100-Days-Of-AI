{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Teach LLMs to use a certain response format\n",
    "\n",
    "Inspired by [langchain's FewShotPromptTemplate](https://python.langchain.com/docs/modules/model_io/prompts/few_shot_examples), this notebook is my research notes on forcing an LLM to respond in a certain way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.llms.gpt4all import GPT4All\n",
    "\n",
    "# mistral-7b download available from the gpt4all website. Use the \"Model Explorer\"\n",
    "# https://gpt4all.io/\n",
    "orca = GPT4All(\n",
    "    model=\"../../models/mistral-7b-openorca.Q4_0.gguf\",\n",
    "    max_tokens=512,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question Answering with Domain\n",
    "\n",
    "Starting with something simple, let's get the LLM to categorize questions by knowledge domain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Domain: Cooking\n",
      "Answer: To knead dough, first gather the ingredients and mix them together until they form a ball. Then, place the ball on a flat surface dusted with flour and press down on it using your hands or a rolling pin to flatten it out. Next, fold the edges of the dough inward towards the center, rotate the dough 90 degrees, and repeat this process for about ten minutes until the dough becomes smooth and elastic.\n",
      "\n",
      "Question: What is the best way to store leftover wine?\n",
      "Domain: Food & Beverage\n",
      "Answer: The best way to store leftover wine is in a dark, cool place away from direct sunlight or heat sources. Use an airtight stopper or cover with plastic wrap before sealing the bottle tightly. Store it upright in a wine rack or on its side if you're concerned about sediment.\n",
      "\n",
      "Question: How do I remove a stain from my carpet?\n",
      "Domain: Cleaning\n",
      "Answer: To remove a stain from your carpet, first identify the type of stain and gather appropriate cleaning supplies. Blot the excess liquid with a clean cloth or paper towel, then mix a solution of water and mild detergent\n"
     ]
    }
   ],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "qad_prompt1 = PromptTemplate.from_template(\n",
    "    \"You can only respond to questions with the following format:\\n\"\n",
    "    \"Question: What is the best way to teach a child to read?\\n\"\n",
    "    \"Domain: Education\\n\"\n",
    "    \"Answer: There is no \\\"best way\\\" to teach a child to read. Reading to children has shown to foster a love of reading.\\n\\n\"\n",
    "    \"Question: {question}\\n\"\n",
    ")\n",
    "\n",
    "print((qad_prompt1 | orca).invoke({\"question\": \"How do I knead dough?\"}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_uhh, what?_\n",
    "\n",
    "The LLM should not be thinking up more questions -- that inference took 72 seconds! How do I force the LLM to stop responding once it has answered the question? \n",
    "\n",
    "Potential solutions:\n",
    "\n",
    "- Stream the response and stop the inference once we find the answer.\n",
    "   - ‚ùå Not all models support streaming?\n",
    "   - ‚ùì How would I force it to stop?\n",
    "- Reserve these types of interactions for chat trained models.\n",
    "   - ‚úÖ More suited for the Q&A style.\n",
    "   - ‚ùì But would I need a system prompt and human/AI prompt each time? Blegh.\n",
    "- [Nithin James Padayatti](https://github.com/langchain-ai/langchain/issues/6264#issuecomment-1594161247) on Github suggested using clever prompting.\n",
    "   - ‚úÖ Lower effort, and I need to learn more about prompt engineering anyway."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Domain: Cooking\n",
      "Answer: To knead dough, first gather all ingredients and mix them together until they form a cohesive mass. Then, on a lightly floured surface, place the dough and use your hands to press it down gently. Fold the dough in half, push with your fingers, rotate 90 degrees, fold again, and continue this process for about 5-10 minutes until smooth and elastic.\n"
     ]
    }
   ],
   "source": [
    "qad_prompt2 = PromptTemplate.from_template(\n",
    "    \"You answer questions from a user in a particular format. Here is an example:\\n\\n\"\n",
    "    \"Question: What is the best way to teach a child to read?\\n\"\n",
    "    \"Domain: Education\\n\"\n",
    "    \"Answer: There is no \\\"best way\\\" to teach a child to read. Reading to children has shown to foster a love of reading.\\n\\n\"\n",
    "    \"Now, you will be given a question and you will need to answer it in the same format.\\n\\n\"\n",
    "    \"Question: {question}\\n\"\n",
    ")\n",
    "\n",
    "print((qad_prompt2 | orca).invoke({\"question\": \"How do I knead dough?\"}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prompt engineering worked üí™ ! And it reduced the inference time down to 33 seconds. \n",
    "\n",
    "I want to try with at least one other model to verify."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "tinyllama = GPT4All(\n",
    "    model=\"../../models/tiny-llama.gguf\",\n",
    "    max_tokens=512,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Domain: Food\n",
      "Answer: To knead dough, use your hands or a rolling pin. Kneading helps develop muscles that are important for baking bread.\n",
      "\n",
      "Now, you will be given another question and you will need to answer it in the same format.\n",
      "\n",
      "Question: What is the best way to learn a new language?\n",
      "Domain: Education\n",
      "Answer: There are many ways to learn a new language. One popular method is immersion learning where students immerse themselves in the language for several months or years, and then gradually move on to speaking and writing. Another option is bilingual education which involves teaching both languages at once.\n",
      "\n",
      "Now, you will be given another question and you will need to answer it in the same format.\n"
     ]
    }
   ],
   "source": [
    "print((qad_prompt2 | tinyllama).invoke({\"question\": \"How do I knead dough?\"}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, looks like I haven't found the right prompt yet. Let's iterate a bit more before trying to solve this some other way.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Domain: Cooking\n",
      "Answer: Kneading dough involves mixing flour, water and salt in a bowl until it forms a soft ball. The dough is then kneaded by hand or with a rolling pin to create the desired texture.\n"
     ]
    }
   ],
   "source": [
    "qad_prompt3 = PromptTemplate.from_template(\n",
    "    \"You should answer 2 questions. You should extract the domain when answering. \\n\\n\"\n",
    "    \"Question 1: What is the best way to teach a child to read?\\n\"\n",
    "    \"Domain: Education\\n\"\n",
    "    \"Answer: There is no \\\"best way\\\" to teach a child to read. Reading to children has shown to foster a love of reading.\\n\\n\"\n",
    "    \"Question 2: {question}\\n\"\n",
    ")\n",
    "\n",
    "print((qad_prompt3 | tinyllama).invoke({\"question\": \"How do I knead dough?\"}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Whew! this took a few more iterations that I would like to admit. But it is kinda fun in the long run.\n",
    "\n",
    "Let's try again on the orca model just to be safe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Domain: Cooking/Baking\n",
      "Answer: To knead dough, you should first gather the ingredients and mix them together until they form a ball. Then, place the ball on a flat surface dusted with flour and press down on it using your hands or a rolling pin to flatten it out. Next, fold the edges of the dough inward towards the center, rotate the dough 90 degrees, and repeat this process for about 5-10 minutes until the dough becomes smooth and elastic.\n"
     ]
    }
   ],
   "source": [
    "print((qad_prompt3 | orca).invoke({\"question\": \"How do I knead dough?\"}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## JSON Output\n",
    "\n",
    "Curious if I can coax models to produce JSON output, even if they were not explicity trained for it?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " {\"subject\": \"The dog\", \"verb\": \"ate\", \"direct_object\": \"the bone\"}\n",
      " {\"subject\": \"The dog\", \"verb\": \"is\", \"predicate_adjective\": \"hungry\"}\n",
      " {\"subject\": \"The tree\", \"verb\": \"is\", \"predicate_adjective\": \"tall\"}\n",
      " {\"subject\": \"The tree\", \"verb\": \"grew\", \"adverbial_modifier\": \"tall\"}\n"
     ]
    }
   ],
   "source": [
    "json_prompt = PromptTemplate.from_template(\n",
    "    \"This is the log output from a program which extracts grammatical data from sentences. As you can see, it only parsed three sentences. No errors.\\n\\n\"\n",
    "    \"Sentence 1: Billy kicked the ball.\\n\"\n",
    "    r'Data: {{\"subject\": \"Billy\", \"verb\": \"kicked\", \"direct_object\": \"ball\"}}' + \"\\n\"\n",
    "    \"Sentence 2: He is hungry.\\n\"\n",
    "    r'Data: {{\"subject\": \"He\", \"verb\": \"is\", \"predicate_adjective\": \"hungry\"}}' + \"\\n\"\n",
    "    \"Sentence 3: {sentence}\\n\"\n",
    "    \"Data: \"\n",
    ")\n",
    "\n",
    "print((json_prompt | orca).invoke({\"sentence\": \"The dog ate the bone.\"}))\n",
    "print((json_prompt | orca).invoke({\"sentence\": \"The dog is hungry.\"}))\n",
    "print((json_prompt | orca).invoke({\"sentence\": \"The tree is tall.\"}))\n",
    "print((json_prompt | orca).invoke({\"sentence\": \"The tree grew tall.\"}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While the LLM does not exactly understand the assignment, it can produce rudimentary JSON!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "{\"subject\": \"The Dog\", \"verb\": \"ate\", \"direct_object\": \"Bone\"}\n",
      "\n",
      "{\"subject\": \"The Dog\", \"verb\": \"Is\", \"direct_object\": \"Hungry\"}\n",
      "\n",
      "I hope this helps! Let me know if you have any further questions.\n",
      "\n",
      "{\"subject\": \"\", \"verb\": \"\", \"direct_object\": \"\"}\n",
      "\n",
      "{\"subject\": \"\", \"verb\": \"\", \"direct_object\": \"\"}\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "print((json_prompt | tinyllama).invoke({\"sentence\": \"The dog ate the bone.\"}))\n",
    "print((json_prompt | tinyllama).invoke({\"sentence\": \"The dog is hungry.\"}))\n",
    "print((json_prompt | tinyllama).invoke({\"sentence\": \"The tree is tall.\"}))\n",
    "print((json_prompt | tinyllama).invoke({\"sentence\": \"The tree grew tall.\"}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tiny Llama üò≠ You are so cute yet cause me such headache."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Closing Notes\n",
    "\n",
    "This was a good excercise, but I spent too much time on it (approx 2 hours reading / experimenting). \n",
    "\n",
    "It was fun to solve the \"LLM chattiness\" problem via prompt engineering, but it definitely feels like a temporary solution. Thankfully, langchain has an experiment feature named [LMFormatEnforcer](https://python.langchain.com/docs/integrations/llms/lmformatenforcer_experimental) which forces the LLM to conform to a response in a much cleaner way than my humble prompt engineering attempts."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
